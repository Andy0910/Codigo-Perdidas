---
title: "Perdidas trab clase"
author: "Andres gonzalez"
date: "1/9/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Ejercicios en clase

## Deducibles

### 4.1 Deducibles de franquicia

```{r}
1-(1-((2000/2500)^3))

x <- 0:5000
fpareto <- dpareto(x, shape = 3, scale = 2000)
plot(x,fpareto)

```

```{r}
n <- 100000
pareto_aleatorio <- rpareto(n, shape = 3, scale = 2000)
hist(pareto_aleatorio, freq = FALSE, breaks = 1000)

```

```{r}
idx <- pareto_aleatorio > 500
YL <- numeric(n)
YL[idx] <- pareto_aleatorio[idx]
YL[!idx] <- 0
hYL <- hist(YL, breaks = 1000, freq = FALSE, xlim = c(0, 5000))
```

```{r}
x <- 1:5000
fYL <- 3 * (2000)^3 / (2000 + x)^4
plot(1:5000, fYL)
points(hYL$mids, hYL$density, col = "red", type = "h")


```
```{R}
YP <- NA
YP[idx] <- pareto_aleatorio[idx]
hYP <- hist(YP, breaks = 5000, freq = FALSE)
```

```{r}
x <- 1:5000
fYP <- 3*(2500)^3/(x+2000)^4
plot(1:5000, fYP)
points(hYP$mids, hYP$density, col = "red", type = "h")

```
```{r}
plot(1:5000, fYP, type = "l", col = "blue")
lines(1:5000, fYL, col = "red")
```

### 4.3 Efecto de la inflacion

```{r}
k<-500/1.1
r<- 0.1

costo.perdida <- (1+r)*( (2000/2) - ((2000/2)*(1-( (2000)^2/((2000+k)^2))  )) )

costo.pago <- costo.perdida / ((2000/(k+2000))^3)

costo.pago

```
### 4.3.5 Limite de poliza
```{r}
n <- 100000
pareto_aleatorio <- rpareto(n, shape = 3, scale = 2000)
hist(pareto_aleatorio, freq = FALSE, breaks = 1000)

```

```{r}
idx <- pareto_aleatorio < 3000
YL <- numeric(n)
YL[idx] <- pareto_aleatorio[idx]
YL[!idx] <- 3000
hYL <- hist(YL, breaks = 1000, freq = FALSE, xlim = c(0, 5000))
```

```{r}
x <- 1:5000
fYL <- 3 * (2000)^3 / (2000 + x)^4
plot(1:5000, fYL)
points(hYL$mids, hYL$density, col = "red", type = "h")
```

#### Valor esperado con inflacion y sin inflacion, limite de poliza

```{r}
k <- 3000/1.1
esp.yu.infl <- (1.1)*(1000 *(1-((2000/(2000+k))^2)) )
esp.yu <- 1000*(1-( (2000/5000)^2  ))

esp.yu.infl
esp.yu
```

### 4.4 Generalizacion de modificaciones

```{r}
#FALTA


```

### 4.5 Impacto deducible frecuencia de reclamos

```{r}


```

## 5.4 Impacto de las pÃ³lizas individuales al modelo agregado

### Ejercicio 5.5

```{r}

N <- read.csv("cap04_ejercicio_5_5_N.csv")
X <- read.csv("cap04_ejercicio_5_5_X.csv")



#Ajustar dist de la N

nn <- data.frame(table(N$x))



n <- 1:9 * nn$Freq[2:length(nn$Var1)] / nn$Freq[1:(length(nn$Var1) -1 )]

plot(n)

n_techo <- data.frame(i = 1:length(n),n)


modelo <- lm(n_techo$n ~ n_techo$i)

a <- modelo$coefficients[1]
b <- modelo$coefficients[2]

```

## Modelo de riesgo individual

### Ejercicio 5.6

```{r}
tabla9.16 <- read.csv(text = "Empleado,Edad,Genero,Beneficio,Mortalidad
1,20,M ,15000,0.00149
2,23,M ,16000,0.00142
3,27,M ,20000,0.00128
4,30,M ,28000,0.00122
5,31,M ,31000,0.00123
6,46,M ,18000,0.00353
7,47,M ,26000,0.00394
8,49,M ,24000,0.00484
9,64,M ,60000,0.02182
10,17,F ,14000,0.0005
11,22,F ,17000,0.0005
12,26,F ,19000,0.00054
13,37,F ,30000,0.00103
14,55,F ,55000,0.00479
")

mu_S <- tabla9.16 %>%
  mutate(prod = Beneficio * Mortalidad) %>%
  summarise(sum(prod))
mu_S

var_S <- tabla9.16 %>%
  mutate(prod = Beneficio^2 * Mortalidad * (1 - Mortalidad)) %>%
  summarise(sum(prod))
var_S

mu_s <- mu_S$`sum(prod)`[1] / 1000
sd_s <- sqrt( var_S$`sum(prod)`[1] / (1000*1000) )


pnorm(mu_s, mean=mu_s, sd = sd_s, lower.tail = F)

plnorm(mu_s,meanlog = -0.89516, sdlog = 1.79733 , lower.tail = F)

#aumento del 45%

pnorm(mu_s * 1.45 , mean=mu_s, sd=sd_s, lower.tail = F)

plnorm(mu_s * 1.45, mean=-0.89516, sd=1.79733,lower.tail = F)

```

## 6.3 Estimador de kaplan-meier y nelson-allen

```{r}
library(tidyverse)
library(actuar)
library(survival)
library(insuranceData)
library(ggfortify)
library(survminer)
library(goftest)

tabla_d <- read.csv(text = "policy,d,obs,event
1,0,0.1,s
2,0,0.5,s
3,0,0.8,s
4,0,0.8,d
5,0,1.8,s
6,0,1.8,s
7,0,2.1,s
8,0,2.5,s
9,0,2.8,s
10,0,2.9,d
11,0,2.9,d
12,0,3.9,s
13,0,4,d
14,0,4,s
15,0,4.1,s
16,0,4.8,d
17,0,4.8,s
18,0,4.8,s
19,0,5,e
20,0,5,e
21,0,5,e
22,0,5,e
23,0,5,e
24,0,5,e
25,0,5,e
26,0,5,e
27,0,5,e
28,0,5,e
29,0,5,e
30,0,5,e
31,0.3,5,e
32,0.7,5,e
33,1,4.1,d
34,1.8,3.1,d
35,2.1,3.9,s
36,2.9,5,e
37,2.9,4.8,s
38,3.2,4,d
39,3.4,5,e
40,3.9,5,e")

#incluir si esta vivo o no
tabla_d <- tabla_d %>%
mutate(delta = case_when(
event == "d" ~ 1,
event %in% c("s", "e") ~ 0
))

#construccion de intervalos
surv1 <- Surv(
time = tabla_d$d,
time2 = tabla_d$obs,
event = tabla_d$delta
)
surv1

#calculo kaplan-meier
kaplan_meier <- survfit(surv1 ~ 1)
summary(kaplan_meier)

autoplot(kaplan_meier)

ggsurvplot(kaplan_meier, data = tabla_d, conf.int = TRUE)

ggsurvplot(kaplan_meier, data = tabla_d, conf.int = TRUE, risk.table = TRUE)

#calculo nelson-allen
nelson_allen <- survfit(surv1 ~ 1, type = "fleming-harrington")
summary(nelson_allen)

autoplot(nelson_allen)

ggsurvplot(nelson_allen, data = tabla_d, conf.int = TRUE)

ggsurvplot(nelson_allen, data = tabla_d, conf.int = TRUE, risk.table = TRUE)

#estimadores combinados
ggsurvplot_combine(
list(kaplan_meier, nelson_allen),
data = tabla_d,
legend.labs = c("Kaplan Meier", "Nelson Allen")
)

ggsurvplot_combine(
list(kaplan_meier, nelson_allen),
data = tabla_d,
legend.labs = c("Kaplan Meier", "Nelson Allen"),
fun = "cumhaz"
)

#Intervalos de confianza
#+- 1.96 VaR(Sn(t))
kaplan_meier_plain <- survfit(surv1 ~ 1, conf.type = "plain")
#utilizando g = log(-log(S))
kaplan_meier_loglog <- survfit(surv1 ~ 1, conf.type = "log-log")

ggsurvplot_combine(
list(kaplan_meier_plain, kaplan_meier_loglog),
data = tabla_d,
legend.labs = c("IC Normal", "IC Log-Log"), conf.int = TRUE
)


```

## 6.5 Estimacion curvas sobrevivencia 

### 6.5.5 Maxima verosimilitud

```{r}
library(flexsurv)

# name = pareto del paquete actuar
custom_pareto <-
list(
name = "pareto",
pars = c("shape", "scale"),
location = c("scale"),
transforms = c(log, log),
inv.transforms = c(exp, exp)
)

#hace la estimacion por max verosim con la funcion pareto que creamos
parametric_pareto <- flexsurvreg(
surv1 ~ 1,
dist = custom_pareto,
inits = c(1, 1)
)

#funcion de S con los parametros calculados
summary(parametric_pareto)

parametric_pareto

plot(parametric_pareto)

#No esta tan bien, entonces repetimos con una gamma
parametric_gamma <- flexsurvreg(
surv1 ~ 1,
dist = "gamma",
inits = c(1, 1)
)

summary(parametric_gamma)

parametric_gamma

plot(parametric_gamma)

#Ejemplo con dataCar

library(fitdistrplus)

data("dataCar")

summary(dataCar)

#describe la forma de distribuciones conocidas con la forma de los datos
descdist(dataCar$claimcst0, discrete=FALSE)

descdist(dataCar$claimcst0[dataCar$clm == 1], discrete = FALSE)

#funcion de sobrevivencia tomando en cuenta todos los reclamos
surv2 <- Surv(time = dataCar$claimcst0)
km_policy <- survfit(surv2 ~ 1)
autoplot(km_policy)
ggsurvplot(km_policy, data = dataCar)

#solo los que se reclamaron
surv3 <- Surv(time = dataCar$claimcst0[dataCar$clm == 1])
km_policy <- survfit(surv3 ~ 1)
autoplot(km_policy)
ggsurvplot(km_policy, data = dataCar)

#ajusta una gamma
data_car_gamma <- flexsurvreg(surv3 ~ 1, dist = "gamma", inits = c(1, 1))
plot(data_car_gamma)

```


## 7

### Ejercicio 7.3 - Maxima Verosimilitud para ajustar los modelos

#### Binomial

```{r}
library(fitdistrplus)
library(actuar)

loglik <- data.frame("dist" = numeric(0), "loglik" = numeric(0))

#Carga de datos
tabla_14_7 <- read.csv("~/andrestemp/tabla_14_7.csv")

#Desagregar datos
tabla_14_7.flat <- rep(tabla_14_7$accidentes, tabla_14_7$polizas)

hist(tabla_14_7.flat)

#Binomial
binomial_loglik <- data.frame(
  size = numeric(0),
  prob = numeric(0),
  loglik = numeric(0)
)

for(s in 6:15){
  fit_binom <- fitdist(
    data = tabla_14_7.flat,
    dist = "binom",
    fix.arg = list(size = s),
    start = list(prob = 0.3)
  )
  binomial_loglik <- rbind(
    binomial_loglik,
    data.frame(
      size = s,
      prob = fit_binom$estimate,
      loglik = -fit_binom$loglik
    )
  )
}



loglik <- rbind(loglik, data.frame(dist = "binom",loglik = min(binomial_loglik$loglik)) )

```

#### ZM Binomial

```{r}
#Zero Modified Binomial
zmbinomial_loglik <- data.frame(
  size = numeric(0),
  prob = numeric(0),
  p0 = numeric(0),
  loglik = numeric(0)
)

for(s in 18:24){
  fit_zmbinom <- fitdist(
    data = tabla_14_7.flat,
    dist = "zmbinom",
    fix.arg = list(size = s),
    start = list(prob = 0.3, p0 = (tabla_14_7$polizas[1] / sum(tabla_14_7$polizas)))
  )
  zmbinomial_loglik <- rbind(
    zmbinomial_loglik,
    data.frame(
      size = s,
      prob = fit_zmbinom$estimate["prob"],
      p0 = fit_zmbinom$estimate["p0"],
      loglik = -fit_zmbinom$loglik
    )
  )
}

loglik <- rbind(loglik, data.frame(dist = "zmbinom",loglik = min(zmbinomial_loglik$loglik)) )

```

#### Binomial neg

```{r}
#Binomial neg
fit_nbinom <- fitdist(
  data = tabla_14_7.flat,
  dist = "nbinom",
  start = list(prob = 0.3, size = 6)
)


loglik <- rbind(loglik, data.frame(dist = "neg binom",loglik = -fit_nbinom$loglik)) 
```

#### ZM Binomial Neg

```{r}
#Zero Modified Binomial Neg
fit_zmnbinom <- fitdist(
  data = tabla_14_7.flat,
  dist = "zmnbinom",
  start = list(prob = 0.3, p0 = (tabla_14_7$polizas[1] / sum(tabla_14_7$polizas)), size = 5)
)


loglik <- rbind(loglik, data.frame(dist = "zm nbinom",loglik = -fit_zmnbinom$loglik)) 

```


#### Geometrica

```{r}
#Geometrica
fit_geom <- fitdist(
  data = tabla_14_7.flat,
  dist = "geom",
  start = list(prob = 0.3)
)

loglik <- rbind(loglik, data.frame(dist = "geom",loglik = -fit_geom$loglik)) 

```
#### ZM Geometrica


```{r}
#Zero Modified Geom

fit_zmgeom <- fitdist(
  data = tabla_14_7.flat,
  dist = "zmgeom",
  start = list(prob = 0.3, p0 = (tabla_14_7$polizas[1] / sum(tabla_14_7$polizas)))
)


loglik <- rbind(loglik, data.frame(dist = "zm geom",loglik = -fit_zmgeom$loglik)) 
```

#### Poisson


```{r}
#Poisson

fit_pois <- fitdist(
  data = tabla_14_7.flat,
  dist = "pois",
  start = list(lambda = 0.3)
)


loglik <- rbind(loglik, data.frame(dist = "pois",loglik = -fit_pois$loglik)) 

```

#### ZM Poisson


```{r}
#Zero Modified Poisson
fit_zmpois <- fitdist(
  data = tabla_14_7.flat,
  dist = "zmpois",
  start = list(lambda = 0.3, p0 = (tabla_14_7$polizas[1] / sum(tabla_14_7$polizas)))
)


loglik <- rbind(loglik, data.frame(dist = "zm pois",loglik = -fit_zmpois$loglik)) 
```

#### Ver cual verosimilitud es menor (tomamos minimos por poner -log likelihood)

```{r}
min_loglik <- loglik[order(loglik$loglik),]$dist[1]
min_loglik

prob_est <- fit_zmgeom$estimate["prob"]
p0_est <- fit_zmgeom$estimate["p0"]

hist(rzmgeom(10000, prob = prob_est, p0 = p0_est  ))
hist(tabla_14_7.flat)
```

## 8 Aplicaciones practicas modelos de frecuencia y severidad

### EJEMPLO

```{r}
library(flexsurv)
library(actuar)
library(survminer)
library(fitdistrplus)

S <- rcompound(
  n = 200,
  model.freq = rpois(lambda = 1.5),
  model.sev = rpareto(shape = 7, scale = 2000),
  SIMPLIFY = FALSE
)

hist(S$severity)

hist(S$frequency)

hist(S$frequency)

severity <- S$severity
severity[severity <= 100] <- NA
severity[severity >= 1000] <- 1000
severity <- na.omit(severity)
hist(severity)

frequency <- S$frequency

delta <- severity != 1000
surv <- Surv(time = rep(100,length(severity)),
             time2 = severity,
             event = delta)

custom_pareto <- list(
  name = "pareto",
  pars = c("shape", "scale"),
  location = c("scale"),
  transforms = c(log, log),
  inv.transforms = c(exp, exp)
)
fit_severity <- flexsurvreg(
  formula = surv ~ 1,
  dist = custom_pareto,
  inits = c(1, 1)
)

fit_frequency <- fitdist(data=frequency, dist = "pois")

x <- seq.int(100, 1000, 100)

severity_discrete <- discretize(
  cdf = ppareto(x,
    shape = fit_severity$coefficients["shape"],
    scale = fit_severity$coefficients["scale"]
  ),
  from = 100,
  to = 1000,
  by = 25
)
severity_discrete

frequency_discrete <- dpois(x = 0:20, lambda = fit_frequency$estimate)
frequency_discrete
```

### Ejercicio 8.1

```{r}
fX <- severity_discrete
pn <- matrix(frequency_discrete,nrow=1)

#Devolverse al cap 5

ncolumnas <- length(pn) - 1
nfilas <- 20
tabla_convolucion <- matrix(0, nrow = nfilas + 1, ncol = ncolumnas + 1)

tabla_convolucion[1, 1] <- 1

for (nc in 1:ncolumnas) {
  for (nf in 1:nfilas) {
    s <- 0
    for (k in seq_len(min(nf, length(fX)))) {
      s <- s + tabla_convolucion[nf - k + 1, nc] * fX[k]
    }
    tabla_convolucion[nf + 1, nc + 1] <- s
  }
}

pn_tabla <- kronecker(matrix(1, nrow = nfilas + 1), pn)

fS <- rowSums(pn_tabla * tabla_convolucion)

plot(0:nfilas, fS, type = "l", xlab = "payment", ylab = "aggregate density")
```

#10 Eventos Extremos

```{r}
library(tidyverse)
library(lubridate)
library(extRemes)
library(fitdistrplus)
library(actuar)

temperature <- read.csv(file = "http://www.dataanalysisclassroom.com/wp-content/uploads/2018/03/cp_temperature.csv")

temperature <- temperature %>%
  filter(Year <= 2017) %>%
  mutate(
    TAVG_C = (TAVG - 32) * 5 / 9,
    TMIN_C = (TMIN - 32) * 5 / 9,
    TMAX_C = (TMAX - 32) * 5 / 9
  )

ggplot(temperature %>% filter(Year >= 1995)) +
  geom_point(aes(as.factor(Month), TAVG_C)) +
  facet_wrap(~Year)


temp <- temperature %>% group_by(Year) %>% summarize(max = max(TAVG_C))

fit.GEV <- fevd(temp$max,type="GEV")

plot(fit.GEV)
summary(fit.GEV)

fit.GUM <- fevd(temp$max,type="Gumbel")
  

plot(fit.GUM)
summary(fit.GUM)

fd <- fitdist(temp$max, "gumbel", start = list(alpha = 10, scale = 1))
fd

#sort(temp$max)
# d <- rgumbel(seq(26,40,0.01), alpha = fd$estimate[1], scale=fd$estimate[2] )
# 
# temp <- data.frame(cbind(temp, d))


p33 <- pgumbel(33,fd$estimate[1],fd$estimate[2],lower.tail=F)


1/p33

qgumbel(1/50,fd$estimate[1],fd$estimate[2],lower.tail=F)
```
```{r}
library(EnvStats)
weib <- fitdist(temp$max, "weibull")
qweibull(1/50,weib$estimate[1],weib$estimate[2],lower.tail=F)

gev <- fitdist(temp$max, "evd", start = list(loc = 10, scale = 0.5, shape = 0, threshold = 0), method = "mge")
```
